#!/usr/bin/env python
"""
Ebbinghausè®°å¿†å¢å¼ºLLM - è°ƒè¯•ç‰ˆæœ¬
åŒ…å«CUDAé”™è¯¯è¯Šæ–­å’Œå†…å­˜ç®¡ç†ä¼˜åŒ–
"""

import sys
import os

# è®¾ç½®CUDAè°ƒè¯•ç¯å¢ƒå˜é‡
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ['TORCH_USE_CUDA_DSA'] = '1'

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import torch
from memollm import EbbinghausLLM
import argparse
import traceback

def check_cuda_status():
    """æ£€æŸ¥CUDAçŠ¶æ€"""
    print("=" * 60)
    print("CUDA çŠ¶æ€æ£€æŸ¥")
    print("=" * 60)
    
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"è®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
        print(f"å½“å‰è®¾å¤‡: {torch.cuda.current_device()}")
        print(f"è®¾å¤‡åç§°: {torch.cuda.get_device_name(0)}")
        
        # æ£€æŸ¥æ˜¾å­˜
        mem_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        mem_allocated = torch.cuda.memory_allocated(0) / 1024**3
        mem_cached = torch.cuda.memory_reserved(0) / 1024**3
        
        print(f"\næ˜¾å­˜ä¿¡æ¯:")
        print(f"æ€»æ˜¾å­˜: {mem_total:.2f} GB")
        print(f"å·²åˆ†é…: {mem_allocated:.2f} GB")
        print(f"å·²ç¼“å­˜: {mem_cached:.2f} GB")
        print(f"å¯ç”¨æ˜¾å­˜: {mem_total - mem_cached:.2f} GB")
        
        # å»ºè®®æ£€æŸ¥
        if mem_total < 8.0:
            print("âš ï¸  è­¦å‘Š: æ˜¾å­˜å¯èƒ½ä¸è¶³ä»¥è¿è¡Œ7Bæ¨¡å‹")
        if mem_total - mem_cached < 6.0:
            print("âš ï¸  è­¦å‘Š: å¯ç”¨æ˜¾å­˜ä¸è¶³ï¼Œå»ºè®®ä½¿ç”¨é‡åŒ–åŠ è½½")

def main():
    parser = argparse.ArgumentParser(description="Ebbinghausè®°å¿†å¢å¼ºLLMè°ƒè¯•ç‰ˆ")
    parser.add_argument("--mode", choices=["baseline", "soft_delete", "sparse_attention"], 
                       default="baseline", help="ç”Ÿæˆæ¨¡å¼")
    parser.add_argument("--prompt", type=str, default="è¯·ç®€å•ä»‹ç»ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
                       help="è¾“å…¥æç¤ºè¯")
    parser.add_argument("--max_tokens", type=int, default=20, 
                       help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    parser.add_argument("--model", type=str, default="Qwen/Qwen2.5-0.5B-Instruct", 
                       help="æ¨¡å‹åç§°")
    parser.add_argument("--compare", action="store_true", 
                       help="å¯¹æ¯”ä¸‰ç§æ¨¡å¼")
    parser.add_argument("--verbose", action="store_true", 
                       help="è¯¦ç»†è¾“å‡º")
    parser.add_argument("--force_exact_length", action="store_true",
                       help="å¼ºåˆ¶ç”Ÿæˆå›ºå®šé•¿åº¦çš„æ–‡æœ¬")
    parser.add_argument("--check_cuda", action="store_true",
                       help="æ£€æŸ¥CUDAçŠ¶æ€")
    
    args = parser.parse_args()
    
    if args.check_cuda:
        check_cuda_status()
        return
    
    print("=" * 80)
    print("EBBINGHAUS è®°å¿†å¢å¼º LLM - è°ƒè¯•ç‰ˆ")
    print("=" * 80)
    
    # CUDAçŠ¶æ€æ£€æŸ¥
    check_cuda_status()
    
    # åˆå§‹åŒ–æ¨¡å‹
    print(f"\nåˆå§‹åŒ–æ¨¡å‹: {args.model}")
    try:
        llm = EbbinghausLLM(args.model)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        print("\nå»ºè®®:")
        print("1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ (å¦‚ Qwen/Qwen2.5-0.5B-Instruct)")
        print("2. æˆ–æ·»åŠ  --model å‚æ•°æŒ‡å®šæ›´å°çš„æ¨¡å‹")
        print("3. ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ˜¾å­˜")
        return
    
    if args.compare:
        # å¯¹æ¯”æ¨¡å¼
        print(f"\nğŸ”„ å¯¹æ¯”ä¸‰ç§æ¨¡å¼ (prompt: {args.prompt})")
        print("=" * 60)
        
        modes = ["baseline", "soft_delete", "sparse_attention"]
        results = {}
        
        for mode in modes:
            print(f"\nğŸ“ {mode.upper()} æ¨¡å¼:")
            print("-" * 40)
            
            # æ¸…ç†æ˜¾å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            try:
                result = llm.generate(
                    args.prompt,
                    max_new_tokens=args.max_tokens,
                    generation_mode=mode,
                    return_attention_weights=(mode != "baseline"),
                    verbose=args.verbose,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    force_exact_length=args.force_exact_length
                )
                
                results[mode] = result
                print(f"ç”Ÿæˆæ–‡æœ¬: {result['generated_text']}")
                print(f"Tokenæ•°: {result['num_tokens']}")
                print(f"è€—æ—¶: {result['generation_time']:.2f}ç§’")
                print(f"é€Ÿåº¦: {result['num_tokens']/result['generation_time']:.2f} tokens/ç§’")
                
                # æ˜¾ç¤ºè®°å¿†ç»Ÿè®¡
                if mode != "baseline" and 'memory_stats' in result:
                    layer_0_stats = result['memory_stats'].get('layer_0', {})
                    if layer_0_stats:
                        avg_retention = layer_0_stats.get('avg_retention', 0)
                        num_tokens = layer_0_stats.get('num_tokens', 0)
                        print(f"è®°å¿†çŠ¶æ€: {num_tokens} tokens, å¹³å‡ä¿æŒç‡ {avg_retention:.4f}")
                
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")
                print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
                traceback.print_exc()
                results[mode] = None
                
                # æä¾›è¯Šæ–­ä¿¡æ¯
                if "CUDA" in str(e):
                    print("\nCUDAé”™è¯¯è¯Šæ–­:")
                    print("1. æ£€æŸ¥æ˜¾å­˜æ˜¯å¦è¶³å¤Ÿ")
                    print("2. å°è¯•ä½¿ç”¨æ›´å°çš„max_tokens")
                    print("3. è€ƒè™‘ä½¿ç”¨é‡åŒ–æ¨¡å‹")
        
        # æ€§èƒ½æ€»ç»“
        print(f"\n{'='*60}")
        print("æ€§èƒ½æ€»ç»“:")
        print(f"{'='*60}")
        
        successful_results = {k: v for k, v in results.items() if v is not None}
        if successful_results:
            for mode in modes:
                if mode in successful_results:
                    speed = successful_results[mode]["num_tokens"] / successful_results[mode]["generation_time"]
                    quality = "âœ…" if len(successful_results[mode]["generated_text"]) > 10 else "âš ï¸"
                    print(f"{mode:15}: {speed:6.2f} tokens/ç§’ {quality}")
                else:
                    print(f"{mode:15}: âŒ å¤±è´¥")
    
    else:
        # å•æ¨¡å¼
        print(f"\nğŸ”„ {args.mode.upper()} æ¨¡å¼ç”Ÿæˆ")
        print(f"Prompt: {args.prompt}")
        print("=" * 60)
        
        try:
            result = llm.generate(
                args.prompt,
                max_new_tokens=args.max_tokens,
                generation_mode=args.mode,
                return_attention_weights=(args.mode != "baseline"),
                verbose=args.verbose,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                force_exact_length=args.force_exact_length
            )
            
            print(f"\nğŸ“ ç”Ÿæˆç»“æœ:")
            print(result['generated_text'])
            
            print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
            print(f"- Tokenæ•°: {result['num_tokens']}")
            print(f"- è€—æ—¶: {result['generation_time']:.2f}ç§’") 
            print(f"- é€Ÿåº¦: {result['num_tokens']/result['generation_time']:.2f} tokens/ç§’")
            
            # è®°å¿†ä¿¡æ¯
            if args.mode != "baseline" and 'memory_stats' in result:
                print(f"\nğŸ§  è®°å¿†ç»Ÿè®¡:")
                for layer_idx in [0, 11, 23]:
                    layer_key = f'layer_{layer_idx}'
                    if layer_key in result['memory_stats']:
                        stats = result['memory_stats'][layer_key]
                        if stats:
                            print(f"  Layer {layer_idx}: {stats['num_tokens']} tokens, "
                                  f"avg retention {stats['avg_retention']:.4f}")
            
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            
            # æä¾›è¯Šæ–­ä¿¡æ¯
            if "CUDA" in str(e):
                print("\nCUDAé”™è¯¯è¯Šæ–­:")
                print("1. æ£€æŸ¥æ˜¾å­˜æ˜¯å¦è¶³å¤Ÿ")
                print("2. å°è¯•ä½¿ç”¨æ›´å°çš„max_tokens")
                print("3. è€ƒè™‘ä½¿ç”¨é‡åŒ–æ¨¡å‹")
                print("4. è¿è¡Œ: python debug_cuda.py è¿›è¡Œè¯¦ç»†è¯Šæ–­")
    
    print(f"\n{'='*80}")
    print("æ¼”ç¤ºå®Œæˆ!")
    print(f"{'='*80}")

if __name__ == "__main__":
    main()