#!/usr/bin/env python3
"""
æ›´å…¬å¹³çš„æ€§èƒ½å¯¹æ¯”ï¼šæ ‡å‡†ç”Ÿæˆ vs è®°å¿†å¢å¼ºç”Ÿæˆ
"""

import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer

class SimplifiedGenerate:
    """ç®€åŒ–çš„ç”ŸæˆåŸºå‡†ï¼Œç”¨äºå…¬å¹³å¯¹æ¯”"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.device = next(model.parameters()).device
    
    @torch.no_grad()
    def generate_baseline(self, input_text, max_new_tokens=50, temperature=0.7):
        """æœ€å°åŒ–çš„ç”Ÿæˆå¾ªç¯ï¼Œä½œä¸ºå…¬å¹³åŸºå‡†"""
        # å‡†å¤‡è¾“å…¥
        inputs = self.tokenizer(input_text, return_tensors="pt").to(self.device)
        input_ids = inputs["input_ids"]
        
        generated_ids = []
        current_ids = input_ids
        past_key_values = None
        
        start_time = time.perf_counter()
        
        # ç®€å•çš„ç”Ÿæˆå¾ªç¯
        for _ in range(max_new_tokens):
            # å‰å‘ä¼ æ’­
            outputs = self.model(
                current_ids[:, -1:] if past_key_values else current_ids,
                past_key_values=past_key_values,
                use_cache=True,
                return_dict=True
            )
            
            past_key_values = outputs.past_key_values
            logits = outputs.logits[0, -1, :]
            
            # ç®€å•é‡‡æ ·
            probs = torch.softmax(logits / temperature, dim=-1)
            next_token_id = torch.multinomial(probs, num_samples=1).item()
            
            if next_token_id == self.tokenizer.eos_token_id:
                break
                
            generated_ids.append(next_token_id)
            current_ids = torch.cat([current_ids, torch.tensor([[next_token_id]], device=self.device)], dim=1)
        
        end_time = time.perf_counter()
        
        return {
            'num_tokens': len(generated_ids),
            'time': end_time - start_time,
            'generated_ids': generated_ids
        }

def fair_performance_comparison():
    """å…¬å¹³çš„æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    
    print("ğŸ”¬ å…¬å¹³æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹
    model_name = "Qwen/Qwen2.5-0.5B-Instruct"
    print(f"åŠ è½½æ¨¡å‹: {model_name}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    # åˆ›å»ºç®€åŒ–ç”Ÿæˆå™¨
    simple_gen = SimplifiedGenerate(model, tokenizer)
    
    # æµ‹è¯•æ–‡æœ¬
    test_text = "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "
    max_tokens = 50
    
    print(f"\næµ‹è¯•é…ç½®:")
    print(f"- æ–‡æœ¬: {test_text}")
    print(f"- ç”Ÿæˆé•¿åº¦: {max_tokens} tokens")
    print(f"- ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹å’Œå‚æ•°")
    print(f"- éƒ½æ˜¯ç®€å•çš„Pythonå¾ªç¯å®ç°")
    
    # 1. æµ‹è¯•ç®€åŒ–åŸºå‡†
    print(f"\nğŸ“Š æµ‹è¯•1: ç®€åŒ–åŸºå‡†ç”Ÿæˆ")
    print("-" * 40)
    
    results_baseline = []
    for i in range(3):
        result = simple_gen.generate_baseline(test_text, max_tokens)
        speed = result['num_tokens'] / result['time']
        results_baseline.append(speed)
        print(f"è¿è¡Œ {i+1}: {result['time']:.3f}s, {speed:.2f} tokens/s")
    
    avg_baseline = sum(results_baseline) / len(results_baseline)
    print(f"å¹³å‡: {avg_baseline:.2f} tokens/s")
    
    # 2. æµ‹è¯•æˆ‘ä»¬çš„è®°å¿†ç”Ÿæˆï¼ˆæ— æ³¨æ„åŠ›ï¼‰
    print(f"\nğŸ“Š æµ‹è¯•2: è®°å¿†ç”Ÿæˆï¼ˆæ— æ³¨æ„åŠ›ï¼‰")
    print("-" * 40)
    
    from ebbinghaus_llm import EbbinghausLLM
    ebbinghaus_llm = EbbinghausLLM(model_name)
    
    results_memory = []
    for i in range(3):
        result = ebbinghaus_llm.generate(
            test_text, 
            max_new_tokens=max_tokens,
            return_attention_weights=False,
            verbose=False
        )
        speed = result['num_tokens'] / result['generation_time']
        results_memory.append(speed)
        print(f"è¿è¡Œ {i+1}: {result['generation_time']:.3f}s, {speed:.2f} tokens/s")
    
    avg_memory = sum(results_memory) / len(results_memory)
    print(f"å¹³å‡: {avg_memory:.2f} tokens/s")
    
    # 3. åˆ†æç»“æœ
    print(f"\nğŸ“ˆ æ€§èƒ½åˆ†æ")
    print("=" * 60)
    print(f"ç®€åŒ–åŸºå‡†: {avg_baseline:.2f} tokens/s")
    print(f"è®°å¿†ç”Ÿæˆ: {avg_memory:.2f} tokens/s")
    print(f"æ€§èƒ½å·®å¼‚: {(avg_memory/avg_baseline - 1)*100:+.1f}%")
    
    if avg_memory < avg_baseline:
        print(f"\nâš ï¸  è®°å¿†ç³»ç»Ÿå¼•å…¥äº† {(1 - avg_memory/avg_baseline)*100:.1f}% çš„é¢å¤–å¼€é”€")
        print("è¿™ä¸»è¦æ¥è‡ª:")
        print("- è®°å¿†æƒé‡è®¡ç®—")
        print("- Cacheæƒé‡åº”ç”¨") 
        print("- è®°å¿†ç®¡ç†é€»è¾‘")
    else:
        print(f"\nâœ… è®°å¿†ç³»ç»Ÿåœ¨ç›¸åŒæ¡ä»¶ä¸‹è¡¨ç°æ›´å¥½ï¼")
    
    # 4. å†…å­˜æ•ˆç‡åˆ†æ
    print(f"\nğŸ’¾ ç†è®ºå†…å­˜æ•ˆç‡åˆ†æ")
    print("=" * 60)
    print("è½¯åˆ é™¤ç­–ç•¥:")
    print("- å†…å­˜ä½¿ç”¨: ç›¸åŒï¼ˆä»å­˜å‚¨å®Œæ•´KV Cacheï¼‰")
    print("- è®¡ç®—å¤æ‚åº¦: ç›¸åŒï¼ˆä»æ˜¯å®Œæ•´çŸ©é˜µè¿ç®—ï¼‰")
    print("- å®é™…æ•ˆæœ: é™ä½æŸäº›ä½ç½®çš„å½±å“æƒé‡")
    print("\nå¦‚æœè¦çœŸæ­£æé«˜æ•ˆç‡ï¼Œéœ€è¦:")
    print("- ç¡¬åˆ é™¤ï¼šç‰©ç†ä¸Šç§»é™¤KV Cacheçš„æŸäº›ä½ç½®")
    print("- ç¨€ç–æ³¨æ„åŠ›ï¼šè·³è¿‡æŸäº›è®¡ç®—")
    print("- é‡åŒ–å‹ç¼©ï¼šå‡å°‘å­˜å‚¨ç²¾åº¦")

if __name__ == "__main__":
    fair_performance_comparison()