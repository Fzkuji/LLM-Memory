#!/usr/bin/env python
"""
Ebbinghausè®°å¿†å¢å¼ºLLM - æ ¸å¿ƒæ¼”ç¤º
æ”¯æŒä¸‰ç§æ¨¡å¼ï¼šbaseline, soft_delete, sparse_attention
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from memollm import EbbinghausLLM
import argparse

def main():
    parser = argparse.ArgumentParser(description="Ebbinghausè®°å¿†å¢å¼ºLLMæ¼”ç¤º")
    parser.add_argument("--mode", choices=["baseline", "soft_delete", "sparse_attention", "sparse_delete"], 
                       default="baseline", help="ç”Ÿæˆæ¨¡å¼")
    parser.add_argument("--prompt", type=str, default="è¯·è¯¦ç»†è§£é‡Šç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å·¥ä½œåŸç†ï¼ŒåŒ…æ‹¬ä»è®­ç»ƒåˆ°æ¨ç†çš„å®Œæ•´æµç¨‹ï¼Œæ¶‰åŠçš„å…³é”®æŠ€æœ¯ï¼Œä»¥åŠå®ƒä»¬æ˜¯å¦‚ä½•ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€çš„ï¼Ÿ",
                       help="è¾“å…¥æç¤ºè¯")
    parser.add_argument("--max_tokens", type=int, default=50, 
                       help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    parser.add_argument("--model", type=str, default="Qwen/Qwen2.5-0.5B-Instruct", 
                       help="æ¨¡å‹åç§°")
    parser.add_argument("--compare", action="store_true", 
                       help="å¯¹æ¯”ä¸‰ç§æ¨¡å¼")
    parser.add_argument("--verbose", action="store_true", 
                       help="è¯¦ç»†è¾“å‡º")
    parser.add_argument("--force_exact_length", action="store_true",
                       help="å¼ºåˆ¶ç”Ÿæˆå›ºå®šé•¿åº¦çš„æ–‡æœ¬ï¼ˆå¿½ç•¥EOSæ ‡è®°ï¼‰")
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("EBBINGHAUS è®°å¿†å¢å¼º LLM")
    print("=" * 80)
    
    # åˆå§‹åŒ–æ¨¡å‹
    print(f"åˆå§‹åŒ–æ¨¡å‹: {args.model}")
    llm = EbbinghausLLM(args.model)
    
    if args.compare:
        # å¯¹æ¯”æ¨¡å¼
        print(f"\nğŸ”„ å¯¹æ¯”å››ç§æ¨¡å¼ (prompt: {args.prompt})")
        print("=" * 60)
        
        modes = ["baseline", "soft_delete", "sparse_attention", "sparse_delete"]
        results = {}
        
        for mode in modes:
            print(f"\nğŸ“ {mode.upper()} æ¨¡å¼:")
            print("-" * 40)
            
            try:
                result = llm.generate(
                    args.prompt,
                    max_new_tokens=args.max_tokens,
                    generation_mode=mode,
                    return_attention_weights=(mode != "baseline"),
                    verbose=args.verbose,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    force_exact_length=args.force_exact_length
                )
                
                results[mode] = result
                print(f"ç”Ÿæˆæ–‡æœ¬: {result['generated_text']}")
                print(f"Tokenæ•°: {result['num_tokens']}")
                print(f"è€—æ—¶: {result['generation_time']:.2f}ç§’")
                print(f"é€Ÿåº¦: {result['num_tokens']/result['generation_time']:.2f} tokens/ç§’")
                
                # æ˜¾ç¤ºè®°å¿†ç»Ÿè®¡å’Œåˆ é™¤ä¿¡æ¯
                if mode != "baseline":
                    if 'memory_stats' in result:
                        layer_0_stats = result['memory_stats'].get('layer_0', {})
                        if layer_0_stats:
                            avg_retention = layer_0_stats.get('avg_retention', 0)
                            num_tokens = layer_0_stats.get('num_tokens', 0)
                            print(f"è®°å¿†çŠ¶æ€: {num_tokens} tokens, å¹³å‡ä¿æŒç‡ {avg_retention:.4f}")
                    
                    # æ˜¾ç¤ºåˆ é™¤ä¿¡æ¯ï¼ˆä»…sparse_deleteæ¨¡å¼ï¼‰
                    if mode == "sparse_delete" and 'total_removed_tokens' in result:
                        print(f"åˆ é™¤tokenæ•°: {result['total_removed_tokens']}")
                
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")
                results[mode] = None
        
        # æ€§èƒ½æ€»ç»“
        print(f"\n{'='*60}")
        print("æ€§èƒ½æ€»ç»“:")
        print(f"{'='*60}")
        
        if all(results.values()):
            for mode in modes:
                speed = results[mode]["num_tokens"] / results[mode]["generation_time"]
                quality = "âœ…" if len(results[mode]["generated_text"]) > 10 else "âš ï¸"
                print(f"{mode:15}: {speed:6.2f} tokens/ç§’ {quality}")
    
    else:
        # å•æ¨¡å¼
        print(f"\nğŸ”„ {args.mode.upper()} æ¨¡å¼ç”Ÿæˆ")
        print(f"Prompt: {args.prompt}")
        print("=" * 60)
        
        try:
            result = llm.generate(
                args.prompt,
                max_new_tokens=args.max_tokens,
                generation_mode=args.mode,
                return_attention_weights=(args.mode != "baseline"),
                verbose=args.verbose,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                force_exact_length=args.force_exact_length
            )
            
            print(f"\nğŸ“ ç”Ÿæˆç»“æœ:")
            print(result['generated_text'])
            
            print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
            print(f"- Tokenæ•°: {result['num_tokens']}")
            print(f"- è€—æ—¶: {result['generation_time']:.2f}ç§’") 
            print(f"- é€Ÿåº¦: {result['num_tokens']/result['generation_time']:.2f} tokens/ç§’")
            
            # è®°å¿†ä¿¡æ¯
            if args.mode != "baseline" and 'memory_stats' in result:
                print(f"\nğŸ§  è®°å¿†ç»Ÿè®¡:")
                for layer_idx in [0, 11, 23]:
                    layer_key = f'layer_{layer_idx}'
                    if layer_key in result['memory_stats']:
                        stats = result['memory_stats'][layer_key]
                        if stats:
                            print(f"  Layer {layer_idx}: {stats['num_tokens']} tokens, "
                                  f"avg retention {stats['avg_retention']:.4f}")
            
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"\n{'='*80}")
    print("æ¼”ç¤ºå®Œæˆ!")
    print(f"{'='*80}")

if __name__ == "__main__":
    main()