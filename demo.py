#!/usr/bin/env python
"""
Ebbinghausè®°å¿†å¢å¼ºLLM - æ ¸å¿ƒæ¼”ç¤º
æ”¯æŒä¸¤ç§æ¨¡å¼ï¼šbaseline, memory_enhanced
memory_enhancedæ¨¡å¼ç°åœ¨ä½¿ç”¨è‡ªç„¶è®°å¿†æƒé‡ï¼ˆ0-1ï¼‰ï¼Œä»…åœ¨æƒé‡ä½äºé˜ˆå€¼æ—¶è¿›è¡Œç¡¬åˆ é™¤
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from memollm import EbbinghausLLM
import argparse

def main():
    parser = argparse.ArgumentParser(description="Ebbinghausè®°å¿†å¢å¼ºLLMæ¼”ç¤º")
    parser.add_argument("--mode", choices=["baseline", "memory_enhanced"], 
                       default="baseline", help="ç”Ÿæˆæ¨¡å¼")
    parser.add_argument("--prompt", type=str, default="è¯·è¯¦ç»†è§£é‡Šç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å·¥ä½œåŸç†ï¼ŒåŒ…æ‹¬ä»è®­ç»ƒåˆ°æ¨ç†çš„å®Œæ•´æµç¨‹ï¼Œæ¶‰åŠçš„å…³é”®æŠ€æœ¯ï¼Œä»¥åŠå®ƒä»¬æ˜¯å¦‚ä½•ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€çš„ï¼Ÿ",
                       help="è¾“å…¥æç¤ºè¯")
    parser.add_argument("--max_tokens", type=int, default=50, 
                       help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    parser.add_argument("--model", type=str, default="Qwen/Qwen2.5-0.5B-Instruct", 
                       help="æ¨¡å‹åç§°")
    parser.add_argument("--compare", action="store_true", 
                       help="å¯¹æ¯”ä¸‰ç§æ¨¡å¼")
    parser.add_argument("--verbose", action="store_true", 
                       help="è¯¦ç»†è¾“å‡º")
    parser.add_argument("--force_exact_length", action="store_true",
                       help="å¼ºåˆ¶ç”Ÿæˆå›ºå®šé•¿åº¦çš„æ–‡æœ¬ï¼ˆå¿½ç•¥EOSæ ‡è®°ï¼‰")
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("EBBINGHAUS è®°å¿†å¢å¼º LLM")
    print("=" * 80)
    
    # åˆå§‹åŒ–æ¨¡å‹
    print(f"åˆå§‹åŒ–æ¨¡å‹: {args.model}")
    llm = EbbinghausLLM(args.model)
    
    if args.compare:
        # å¯¹æ¯”æ¨¡å¼
        print(f"\nğŸ”„ å¯¹æ¯”ä¸¤ç§æ¨¡å¼ (prompt: {args.prompt})")
        print("=" * 60)
        
        modes = ["baseline", "memory_enhanced"]
        results = {}
        
        for mode in modes:
            print(f"\nğŸ“ {mode.upper()} æ¨¡å¼:")
            print("-" * 40)
            
            try:
                # ä¸ºmemory_enhancedæ¨¡å¼ä½¿ç”¨æ›´åˆç†çš„åˆ é™¤é˜ˆå€¼
                kwargs = {}
                if mode == "memory_enhanced":
                    kwargs['hard_delete_threshold'] = 0.05  # ä½¿ç”¨æ›´åˆç†çš„é˜ˆå€¼æ¥è§¦å‘åˆ é™¤
                
                result = llm.generate(
                    args.prompt,
                    max_new_tokens=args.max_tokens,
                    generation_mode=mode,
                    return_attention_weights=(mode != "baseline"),
                    verbose=args.verbose,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    force_exact_length=args.force_exact_length,
                    **kwargs
                )
                
                results[mode] = result
                print(f"ç”Ÿæˆæ–‡æœ¬: {result['generated_text']}")
                print(f"Tokenæ•°: {result['num_tokens']}")
                print(f"è€—æ—¶: {result['generation_time']:.2f}ç§’")
                print(f"é€Ÿåº¦: {result['num_tokens']/result['generation_time']:.2f} tokens/ç§’")
                
                # æ˜¾ç¤ºè®°å¿†ç»Ÿè®¡å’Œåˆ é™¤ä¿¡æ¯
                if mode != "baseline":
                    if 'memory_stats' in result:
                        # æ”¶é›†æ‰€æœ‰å±‚çš„ç»Ÿè®¡ä¿¡æ¯
                        all_retentions = []
                        all_token_counts = []
                        
                        for layer_idx in range(24):  # å‡è®¾24å±‚
                            layer_key = f'layer_{layer_idx}'
                            if layer_key in result['memory_stats']:
                                stats = result['memory_stats'][layer_key]
                                if stats and 'num_tokens' in stats and 'avg_retention' in stats:
                                    all_retentions.append(stats['avg_retention'])
                                    all_token_counts.append(stats['num_tokens'])
                        
                        # è®¡ç®—æ•´ä½“ç»Ÿè®¡
                        if all_retentions:
                            avg_retention = sum(all_retentions) / len(all_retentions)
                            avg_tokens = sum(all_token_counts) / len(all_token_counts)
                            print(f"è®°å¿†çŠ¶æ€: å¹³å‡{avg_tokens:.1f} tokens, å¹³å‡ä¿æŒç‡ {avg_retention:.4f}")
                    
                    # æ˜¾ç¤ºmemory_enhancedæ¨¡å¼çš„cacheåˆ é™¤ç‡
                    if mode == "memory_enhanced":
                        cache_deletion_percentage = result.get('cache_deletion_percentage', 0)
                        total_expected_tokens = result.get('total_expected_tokens', 0)
                        total_actual_tokens = result.get('total_actual_tokens', 0)
                        
                        if cache_deletion_percentage > 0:
                            total_cache_deletions = result.get('total_cache_deletions', 0)
                            total_cache_entries = result.get('total_cache_entries', 0)
                            min_cache = result.get('min_cache_length', 0)
                            max_cache = result.get('max_cache_length', 0)
                            print(f"ğŸ—‘ï¸  Cacheåˆ é™¤ç‡: {cache_deletion_percentage:.2f}% ({total_cache_deletions}/{total_cache_entries})")
                            if min_cache != max_cache:
                                print(f"ğŸ”€  æ¯å±‚ç‹¬ç«‹: æœ€çŸ­{min_cache}ä¸ªtoken, æœ€é•¿{max_cache}ä¸ªtoken")
                        
                        print(f"ğŸ“Š Tokens: æœŸæœ›{total_expected_tokens}, å®é™…å¹³å‡{total_actual_tokens:.1f}")
                        
                        # æ˜¾ç¤ºåˆ é™¤äº‹ä»¶æ•°é‡
                        deletion_events = result.get('deletion_events', [])
                        if deletion_events:
                            print(f"ğŸ“‹ åˆ é™¤äº‹ä»¶: {len(deletion_events)} æ¬¡")
                    
                
            except Exception as e:
                print(f"âŒ {mode}æ¨¡å¼é”™è¯¯: {e}")
                results[mode] = None
        
        # æ€§èƒ½æ€»ç»“
        print(f"\n{'='*60}")
        print("æ€§èƒ½æ€»ç»“:")
        print(f"{'='*60}")
        
        if all(results.values()):
            for mode in modes:
                speed = results[mode]["num_tokens"] / results[mode]["generation_time"]
                quality = "âœ…" if len(results[mode]["generated_text"]) > 10 else "âš ï¸"
                print(f"{mode:15}: {speed:6.2f} tokens/ç§’ {quality}")
    
    else:
        # å•æ¨¡å¼
        print(f"\nğŸ”„ {args.mode.upper()} æ¨¡å¼ç”Ÿæˆ")
        print(f"Prompt: {args.prompt}")
        print("=" * 60)

        # ä¸ºmemory_enhancedæ¨¡å¼ä½¿ç”¨æ›´åˆç†çš„åˆ é™¤é˜ˆå€¼
        kwargs = {}
        if args.mode == "memory_enhanced":
            kwargs['hard_delete_threshold'] = 0.05  # æµ‹è¯•é˜ˆå€¼ä¸º0çš„æƒ…å†µ

        result = llm.generate(
            args.prompt,
            max_new_tokens=args.max_tokens,
            generation_mode=args.mode,
            return_attention_weights=(args.mode != "baseline"),
            verbose=args.verbose,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            force_exact_length=args.force_exact_length,
            **kwargs
        )

        print(f"\nğŸ“ ç”Ÿæˆç»“æœ:")
        print(result['generated_text'])

        print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
        print(f"- Tokenæ•°: {result['num_tokens']}")
        print(f"- è€—æ—¶: {result['generation_time']:.2f}ç§’")
        print(f"- é€Ÿåº¦: {result['num_tokens']/result['generation_time']:.2f} tokens/ç§’")

        # è®°å¿†ä¿¡æ¯ - ç›´æ¥åŸºäºcacheé•¿åº¦ç»Ÿè®¡
        if args.mode == "memory_enhanced":
            layer_cache_lengths = result.get('layer_cache_lengths', [])
            if layer_cache_lengths:
                print(f"\nğŸ§  è®°å¿†ç»Ÿè®¡ (åŸºäºå®é™…cacheé•¿åº¦):")

                # è®¡ç®—æ•´ä½“ç»Ÿè®¡
                avg_length = sum(layer_cache_lengths) / len(layer_cache_lengths)
                min_length = min(layer_cache_lengths)
                max_length = max(layer_cache_lengths)

                print(f"  å¹³å‡cacheé•¿åº¦: {avg_length:.1f} tokens")
                print(f"  cacheé•¿åº¦èŒƒå›´: {min_length}-{max_length} tokens")

                # æ‰¾å‡ºcacheæœ€çŸ­å’Œæœ€é•¿çš„å±‚
                min_idx = layer_cache_lengths.index(min_length)
                max_idx = layer_cache_lengths.index(max_length)

                print(f"  cacheæœ€çŸ­å±‚: Layer {min_idx} ({min_length} tokens)")
                print(f"  cacheæœ€é•¿å±‚: Layer {max_idx} ({max_length} tokens)")

                # æ˜¾ç¤ºæ‰€æœ‰å±‚çš„cacheé•¿åº¦
                print(f"\n  å„å±‚cacheé•¿åº¦è¯¦æƒ…:")
                for i in range(0, len(layer_cache_lengths), 6):  # æ¯è¡Œæ˜¾ç¤º6å±‚
                    layer_group = []
                    for j in range(i, min(i+6, len(layer_cache_lengths))):
                        layer_group.append(f"L{j}:{layer_cache_lengths[j]}")
                    print(f"    {' '.join(layer_group)}")

        # æ˜¾ç¤ºmemory_enhancedæ¨¡å¼çš„è¯¦ç»†cacheåˆ é™¤ä¿¡æ¯
        if args.mode == "memory_enhanced":
            cache_deletion_percentage = result.get('cache_deletion_percentage', 0)
            total_expected_tokens = result.get('total_expected_tokens', 0)
            total_actual_tokens = result.get('total_actual_tokens', 0)
            layer_cache_lengths = result.get('layer_cache_lengths', [])

            print(f"\nğŸ“Š Tokenç»Ÿè®¡:")
            print(f"  æœŸæœ›tokens: {total_expected_tokens}")
            print(f"  å®é™…å¹³å‡tokens: {total_actual_tokens:.1f}")

            # æ˜¾ç¤ºæ¯å±‚cacheé•¿åº¦çš„åˆ†å¸ƒ
            if layer_cache_lengths:
                min_cache = result.get('min_cache_length', min(layer_cache_lengths))
                max_cache = result.get('max_cache_length', max(layer_cache_lengths))
                print(f"  å„å±‚cacheé•¿åº¦: æœ€å°{min_cache}, æœ€å¤§{max_cache}")

                # æ˜¾ç¤ºå‰å‡ å±‚å’Œåå‡ å±‚çš„cacheé•¿åº¦
                if len(layer_cache_lengths) >= 6:
                    print(f"  å‰3å±‚cache: {layer_cache_lengths[:3]}")
                    print(f"  å3å±‚cache: {layer_cache_lengths[-3:]}")

                # æ˜¾ç¤ºç¼“å­˜ä¸ä¸€è‡´æ€§ï¼ˆæ¯å±‚ç‹¬ç«‹ç®¡ç†çš„ç‰¹å¾ï¼‰
                unique_lengths = len(set(layer_cache_lengths))
                if min_cache != max_cache:
                    print(f"  ğŸ”€ æ¯å±‚ç‹¬ç«‹ç®¡ç†: {unique_lengths}ç§ä¸åŒé•¿åº¦, èŒƒå›´{min_cache}-{max_cache}")
                else:
                    print(f"  âš ï¸  æ‰€æœ‰å±‚é•¿åº¦ç›¸åŒ({min_cache}) - å¯èƒ½éœ€è¦æ›´é«˜åˆ é™¤é˜ˆå€¼æˆ–transformerså…¼å®¹æ€§é™åˆ¶")

            if cache_deletion_percentage > 0:
                print(f"\nğŸ—‘ï¸  Cacheåˆ é™¤ç»Ÿè®¡:")
                total_cache_deletions = result.get('total_cache_deletions', 0)
                total_cache_entries = result.get('total_cache_entries', 0)
                min_cache = result.get('min_cache_length', 0)
                max_cache = result.get('max_cache_length', 0)

                print(f"  Cacheåˆ é™¤ç‡: {cache_deletion_percentage:.2f}%")
                print(f"  Cacheç»Ÿè®¡: {total_cache_deletions}/{total_cache_entries} æ¡ç›®è¢«åˆ é™¤")
                if min_cache != max_cache:
                    print(f"  æ¯å±‚ç‹¬ç«‹ç®¡ç†: æœ€çŸ­{min_cache}ä¸ªtoken, æœ€é•¿{max_cache}ä¸ªtoken")

                # æ˜¾ç¤ºåˆ é™¤äº‹ä»¶è¯¦æƒ…
                deletion_events = result.get('deletion_events', [])
                if deletion_events:
                    print(f"  åˆ é™¤äº‹ä»¶: {len(deletion_events)} æ¬¡")
                    total_deletions = sum(e.get('total_deletions', 0) for e in deletion_events)
                    avg_cache_per_event = total_cache_deletions / len(deletion_events) if deletion_events else 0
                    print(f"  æ€»å…±åˆ é™¤cacheæ¡ç›®: {total_deletions} ä¸ª")
                    print(f"  å¹³å‡æ¯æ¬¡åˆ é™¤cache: {avg_cache_per_event:.1f} æ¡ç›®")

                    # æ˜¾ç¤ºæ¯å±‚åˆ é™¤å·®å¼‚
                    print(f"  ğŸ” æ¯å±‚åˆ é™¤å·®å¼‚:")
                    total_per_layer = [0] * 24  # å‡è®¾24å±‚
                    for event in deletion_events:
                        per_layer = event.get('per_layer_deletions', [])
                        for i, count in enumerate(per_layer):
                            if i < len(total_per_layer):
                                total_per_layer[i] += count

                    # æ˜¾ç¤ºå·®å¼‚æœ€å¤§çš„å‡ å±‚
                    layer_diffs = [(i, count) for i, count in enumerate(total_per_layer) if count > 0]
                    layer_diffs.sort(key=lambda x: x[1], reverse=True)

                    if layer_diffs:
                        top_layers = layer_diffs[:5]  # æ˜¾ç¤ºåˆ é™¤æœ€å¤šçš„5å±‚
                        layer_info = [f"L{idx}:{count}" for idx, count in top_layers]
                        print(f"    åˆ é™¤æœ€å¤šçš„å±‚: {', '.join(layer_info)}")
            else:
                print(f"\nğŸ—‘ï¸  æ²¡æœ‰è§¦å‘cacheåˆ é™¤ï¼ˆæ‰€æœ‰tokenæƒé‡é«˜äºé˜ˆå€¼ï¼‰")

    print(f"\n{'='*80}")
    print("æ¼”ç¤ºå®Œæˆ!")
    print(f"{'='*80}")

if __name__ == "__main__":
    main()