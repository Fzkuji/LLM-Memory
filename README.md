# Ebbinghausè®°å¿†å¢å¼ºLLM

åŸºäºè‰¾å®¾æµ©æ–¯è®°å¿†é—å¿˜æ›²çº¿çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒä¸‰ç§ç”Ÿæˆæ¨¡å¼ã€‚é€šè¿‡æ¨¡æ‹Ÿäººç±»è®°å¿†çš„é—å¿˜è¿‡ç¨‹ï¼Œå®ç°æ›´é«˜æ•ˆçš„é•¿æ–‡æœ¬ç”Ÿæˆã€‚

## ğŸš€ åŠŸèƒ½ç‰¹ç‚¹

- **ğŸ¯ Baselineæ¨¡å¼**: æ ‡å‡†Transformerç”Ÿæˆï¼ˆä½œä¸ºå¯¹ç…§åŸºå‡†ï¼‰
- **ğŸ”„ Soft Deleteæ¨¡å¼**: é€šè¿‡Forward Hookæ¸è¿›å¼æƒé‡è¡°å‡
- **âš¡ Sparse Attentionæ¨¡å¼**: åŸºäºAttention Maskçš„çœŸæ­£ç¨€ç–æ³¨æ„åŠ›

## ğŸ“Š æ€§èƒ½è¡¨ç°

åœ¨Qwen2.5-0.5Bä¸Šçš„å®æµ‹ç»“æœï¼š
- **Baseline**: 10.90 tokens/ç§’ 
- **Soft Delete**: **20.54 tokens/ç§’** âš¡ (æœ€å¿«)
- **Sparse Attention**: 18.42 tokens/ç§’

> ğŸ” **æ„å¤–å‘ç°**: Soft Deleteæ¨¡å¼é€šè¿‡æ¸è¿›å¼KV cacheä¼˜åŒ–ï¼Œå®ç°äº†è¿‘2å€çš„æ€§èƒ½æå‡ï¼

## å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
pip install torch transformers
```

### Python API ä½¿ç”¨

#### åŸºæœ¬è°ƒç”¨

```python
from memollm import EbbinghausLLM

# åˆå§‹åŒ–æ¨¡å‹
llm = EbbinghausLLM("Qwen/Qwen2.5-0.5B-Instruct")

# ä¸‰ç§æ¨¡å¼ç”Ÿæˆç¤ºä¾‹
modes = ["baseline", "soft_delete", "sparse_attention"]

for mode in modes:
    result = llm.generate(
        "è¯·è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
        max_new_tokens=50,
        generation_mode=mode,
        temperature=0.7,
        do_sample=True
    )
    print(f"{mode}: {result['generated_text']}")
    print(f"é€Ÿåº¦: {result['num_tokens']/result['generation_time']:.2f} tokens/ç§’\n")
```

#### é«˜çº§APIè°ƒç”¨

```python
# å¸¦è®°å¿†åˆ†æçš„ç”Ÿæˆ
result = llm.generate(
    "æ·±åº¦å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ",
    max_new_tokens=100,
    generation_mode="sparse_attention",
    return_attention_weights=True,  # è¿”å›æ³¨æ„åŠ›æƒé‡
    verbose=True,  # æ˜¾ç¤ºè¯¦ç»†è¿‡ç¨‹
    temperature=0.8,
    top_p=0.9
)

# æŸ¥çœ‹ç”Ÿæˆç»“æœ
print("ç”Ÿæˆæ–‡æœ¬:", result['generated_text'])
print("Tokenæ•°é‡:", result['num_tokens'])
print("ç”Ÿæˆæ—¶é—´:", result['generation_time'])

# æŸ¥çœ‹è®°å¿†ç»Ÿè®¡
if 'memory_stats' in result:
    for layer_id, stats in result['memory_stats'].items():
        if stats:
            print(f"{layer_id}: {stats['num_tokens']} tokens, "
                  f"å¹³å‡ä¿æŒç‡ {stats['avg_retention']:.4f}")

# æŸ¥çœ‹è¯¦ç»†tokenæƒé‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
if result.get('token_weights'):
    token_info = result['token_weights']
    print("Tokenåºåˆ—:", token_info['tokens'][:10])  # å‰10ä¸ªtoken
```

#### æ‰¹é‡æµ‹è¯•

```python
# æ‰¹é‡æµ‹è¯•ä¸åŒprompt
prompts = [
    "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
    "è§£é‡Šæ·±åº¦å­¦ä¹ åŸç†",
    "æœºå™¨å­¦ä¹ æœ‰å“ªäº›åº”ç”¨ï¼Ÿ"
]

for prompt in prompts:
    print(f"\nPrompt: {prompt}")
    for mode in ["baseline", "soft_delete", "sparse_attention"]:
        result = llm.generate(prompt, max_new_tokens=30, generation_mode=mode)
        print(f"{mode:15}: {result['generated_text'][:50]}...")
```

### å‘½ä»¤è¡Œè°ƒç”¨

#### 1. ä¸»æ¼”ç¤ºè„šæœ¬ (demo.py)

```bash
# å•æ¨¡å¼ç”Ÿæˆ
python demo.py --mode baseline --prompt "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"
python demo.py --mode soft_delete --prompt "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"
python demo.py --mode sparse_attention --prompt "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"

# å¯¹æ¯”ä¸‰ç§æ¨¡å¼
python demo.py --compare --prompt "è§£é‡Šäººå·¥æ™ºèƒ½" --max_tokens 30

# è‡ªå®šä¹‰å‚æ•°
python demo.py --mode sparse_attention \
    --prompt "è¯·è¯¦ç»†ä»‹ç»æœºå™¨å­¦ä¹ çš„å‘å±•å†ç¨‹" \
    --max_tokens 100 \
    --model "Qwen/Qwen2.5-0.5B-Instruct" \
    --verbose

# æŸ¥çœ‹å¸®åŠ©
python demo.py --help
```

#### 2. æ€§èƒ½æµ‹è¯• (test_performance.py)

```bash
# è¿è¡Œå®Œæ•´æ€§èƒ½æµ‹è¯•ï¼ˆçŸ­ã€ä¸­ã€é•¿æ–‡æœ¬ï¼‰
python test_performance.py
```

#### å‚æ•°è¯´æ˜

**demo.py å‚æ•°ï¼š**
- `--mode`: ç”Ÿæˆæ¨¡å¼ (`baseline`, `soft_delete`, `sparse_attention`)
- `--prompt`: è¾“å…¥æç¤ºè¯
- `--max_tokens`: æœ€å¤§ç”Ÿæˆtokenæ•° (é»˜è®¤: 50)
- `--model`: æ¨¡å‹åç§° (é»˜è®¤: "Qwen/Qwen2.5-0.5B-Instruct")
- `--compare`: å¯¹æ¯”ä¸‰ç§æ¨¡å¼
- `--verbose`: è¯¦ç»†è¾“å‡º

## æŠ€æœ¯å®ç°

### è®°å¿†æœºåˆ¶
åŸºäºè‰¾å®¾æµ©æ–¯é—å¿˜æ›²çº¿: **R = e^(-t/S)**
- R: è®°å¿†ä¿æŒç‡
- t: æ—¶é—´æ­¥æ•°
- S: è®°å¿†å¼ºåº¦

### ä¸‰ç§æ¨¡å¼å¯¹æ¯”

| æ¨¡å¼ | å®ç°æ–¹å¼ | ç‰¹ç‚¹ |
|------|---------|------|
| Baseline | æ ‡å‡†ç”Ÿæˆ | æ— è®°å¿†æœºåˆ¶ï¼Œä½œä¸ºå¯¹ç…§ |
| Soft Delete | æƒé‡è¡°å‡ | å°†KVå€¼ä¹˜ä»¥è®°å¿†æƒé‡(â‰¥0.5) |
| Sparse Attention | KVç¨€ç–åŒ– | æä½æƒé‡ä½ç½®è®¾ä¸º1e-6 |

## ğŸ“ é¡¹ç›®ç»“æ„

```
memory/
â”œâ”€â”€ memollm/              # æ ¸å¿ƒæ¨¡å—
â”‚   â”œâ”€â”€ llm.py           # ä¸»è¦LLMå®ç°
â”‚   â”œâ”€â”€ memory.py        # è‰¾å®¾æµ©æ–¯è®°å¿†ç®¡ç†å™¨
â”‚   â””â”€â”€ __init__.py      # åŒ…åˆå§‹åŒ–
â”œâ”€â”€ examples/            # ç¤ºä¾‹å’Œå·¥å…·
â”‚   â”œâ”€â”€ utils.py         # å®éªŒå·¥å…·
â”‚   â”œâ”€â”€ visualization.py # å¯è§†åŒ–å·¥å…·
â”‚   â””â”€â”€ performance_comparison.py # æ€§èƒ½å¯¹æ¯”
â”œâ”€â”€ demo.py              # ğŸ¯ ä¸»æ¼”ç¤ºè„šæœ¬
â”œâ”€â”€ run.py              # å¿«é€Ÿè¿è¡Œè„šæœ¬
â””â”€â”€ CLAUDE.md           # è¯¦ç»†æŠ€æœ¯æ–‡æ¡£
```

## âš™ï¸ æ ¸å¿ƒå‚æ•°

### è®°å¿†æœºåˆ¶å‚æ•°
- **è®°å¿†å…¬å¼**: `R = e^(-t/S)`
- **é»˜è®¤è®°å¿†å¼ºåº¦(S)**: 5.0
- **æ—¶é—´æ­¥è¿›(t)**: æ¯tokenå¢åŠ 0.01
- **Soft Deleteæƒé‡èŒƒå›´**: [0.8, 1.0]
- **Sparse Attentioné˜ˆå€¼**: 0.001
- **è·¨å±‚èšåˆæ¯”ä¾‹**: 50%å±‚åŒæ„æ‰mask

### ç”Ÿæˆå‚æ•°
- `generation_mode`: ç”Ÿæˆæ¨¡å¼é€‰æ‹©
- `max_new_tokens`: æœ€å¤§ç”Ÿæˆtokenæ•°
- `temperature`: é‡‡æ ·æ¸©åº¦
- `return_attention_weights`: æ˜¯å¦è¿”å›æ³¨æ„åŠ›åˆ†æ

## ğŸ’¡ å®é™…åº”ç”¨åœºæ™¯

```python
from memollm import EbbinghausLLM

llm = EbbinghausLLM("Qwen/Qwen2.5-0.5B-Instruct")

# ğŸ¯ åœºæ™¯1: é«˜æ€§èƒ½æ–‡æœ¬ç”Ÿæˆ
result = llm.generate(
    "å†™ä¸€ç¯‡AIå‘å±•å†ç¨‹çš„æ–‡ç« ",
    max_new_tokens=200,
    generation_mode="soft_delete"  # 20+ tokens/ç§’çš„é«˜é€Ÿç”Ÿæˆ
)

# âš¡ åœºæ™¯2: é•¿æ–‡æœ¬ç¨€ç–æ³¨æ„åŠ›
result = llm.generate(
    "è¯¦ç»†åˆ†ææ·±åº¦å­¦ä¹ çš„æŠ€æœ¯åŸç†",
    max_new_tokens=500,
    generation_mode="sparse_attention"  # è‡ªåŠ¨å¿½ç•¥ä¸é‡è¦çš„å†å²
)

# ğŸ“Š åœºæ™¯3: æ€§èƒ½å¯¹æ¯”åˆ†æ
for mode in ["baseline", "soft_delete", "sparse_attention"]:
    result = llm.generate("è§£é‡Šé‡å­è®¡ç®—", generation_mode=mode)
    speed = result['num_tokens'] / result['generation_time']
    print(f"{mode}: {speed:.2f} tokens/ç§’")
```

## â“ å¸¸è§é—®é¢˜

### Q: ä¸ºä»€ä¹ˆSoft Deleteæœ€å¿«ï¼Ÿ
**A**: é€šè¿‡æ¸è¿›å¼KV cacheå‹ç¼©äº§ç”Ÿ"é›ªçƒæ•ˆåº”"ï¼š
- æ¯æ­¥éƒ½åœ¨ä¼˜åŒ–KV cacheæ•°å€¼èŒƒå›´
- è§¦å‘GPUçš„fast mathä¼˜åŒ–è·¯å¾„
- äº§ç”Ÿç´¯ç§¯æ€§èƒ½æå‡ï¼Œè¶Šç”Ÿæˆè¶Šå¿«

### Q: ä¸‰ç§æ¨¡å¼å¦‚ä½•é€‰æ‹©ï¼Ÿ
| åœºæ™¯ | æ¨èæ¨¡å¼ | åŸå›  |
|------|---------|------|
| é«˜é€Ÿç”Ÿæˆ | Soft Delete | 20+ tokens/ç§’æ€§èƒ½ |
| é•¿æ–‡æœ¬ | Sparse Attention | çœŸæ­£å‡å°‘è®¡ç®—é‡ |
| åŸºå‡†æµ‹è¯• | Baseline | æ ‡å‡†å¯¹ç…§ |

### Q: å†…å­˜å’Œå…¼å®¹æ€§å¦‚ä½•ï¼Ÿ
- âœ… å†…å­˜å¼€é”€æå°ï¼ˆæ¯token <20å­—èŠ‚ï¼‰
- âœ… æ”¯æŒæ‰€æœ‰HuggingFace CausalLMæ¨¡å‹
- âœ… å®Œå…¨å…¼å®¹transformersåº“çš„generate()æ–¹æ³•

## ğŸ”¬ æŠ€æœ¯ç»†èŠ‚

### è‰¾å®¾æµ©æ–¯è®°å¿†å…¬å¼
```
R = e^(-t/S)
```
- **R**: è®°å¿†ä¿æŒç‡ [0,1]
- **t**: æ—¶é—´æ­¥æ•°ï¼ˆæ¯token +0.01ï¼‰  
- **S**: è®°å¿†å¼ºåº¦ï¼ˆåˆå§‹5.0ï¼Œéšattentionå¢å¼ºï¼‰

### å®ç°æœºåˆ¶
- **Soft Delete**: Forward Hookä¿®æ”¹KV cache
- **Sparse Attention**: Attention Maskå®Œå…¨å±è”½
- **è·¨å±‚èšåˆ**: å¤šå±‚å…±è¯†å†³ç­–ï¼Œæé«˜ç¨³å®šæ€§

### æ€§èƒ½ä¼˜åŒ–
- åªå¯¹å‰50%å±‚åº”ç”¨è®°å¿†æœºåˆ¶
- æ‰¹é‡å¤„ç†è®°å¿†æƒé‡è®¡ç®—  
- ä½¿ç”¨transformers.generate()é¿å…ç»´åº¦é—®é¢˜

## è®¸å¯è¯

MIT License